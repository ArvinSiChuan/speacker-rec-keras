{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Conv Speaker Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model proto phase of this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Library for hdf5\n",
    "import h5py as h5\n",
    "\n",
    "# Vis Tool of IPythin\n",
    "from IPython.display import SVG\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "# Useful Magics\n",
    "% reload_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.input_feature import AudioDataset,Compose,CMVN,Feature_Cube,ToOutput\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = './dataset_path'\n",
    "audio_dir = '/'\n",
    "dataset_origin = AudioDataset(files_path=files_path, audio_dir=audio_dir,\n",
    "                           transform=Compose([CMVN(), Feature_Cube(cube_shape=(20, 80, 40), augmentation=True), ToOutput()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_dataset = dataset_origin.__len__()\n",
    "print(length_of_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Basic infos\n",
    "final_file = 'dataset.h5'\n",
    "num_classes = 695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file = None\n",
    "\n",
    "try:\n",
    "    # Try to create H5 file\n",
    "    h5_file = h5.File(final_file, 'w-')\n",
    "except IOError as e:\n",
    "    override = input(\"File exists, override?(Y/N)\").lower()\n",
    "    override = override == 'y' or override == ''\n",
    "    if override:\n",
    "        # Override H5 file\n",
    "        h5_file = h5.File(final_file, 'w')\n",
    "    else:\n",
    "        h5_file = None\n",
    "        print('aborted for not overriding.')\n",
    "except OSError as e:\n",
    "    print(\"EXCEPTION: %s.\" % (e))\n",
    "    print(\"File may be not accessiable!\")\n",
    "    print(\"Saving H5 File(%s) Failed in path %s\" % (save_name, save_path))\n",
    "    if h5_file:\n",
    "        h5_file.close()\n",
    "\n",
    "\n",
    "basic_features_dataset_shape = (0, 20, 80, 40, 1)\n",
    "max_features_dataset_shape = (length_of_dataset+1024, 20, 80, 40, 1)\n",
    "\n",
    "# Create Dataset for images\n",
    "features_dataset = h5_file.create_dataset(\n",
    "    'features',\n",
    "    basic_features_dataset_shape,\n",
    "    dtype=np.float,\n",
    "    chunks=True,\n",
    "    maxshape=max_features_dataset_shape,\n",
    "    compression=None)\n",
    "\n",
    "basic_labels_dataset_shape = (0, num_classes)\n",
    "max_labels_dataset_shape = (length_of_dataset+1024, num_classes)\n",
    "labels_dataset = h5_file.create_dataset(\n",
    "    'labels',\n",
    "    basic_labels_dataset_shape,\n",
    "    dtype=np.int,\n",
    "    chunks=True,\n",
    "    maxshape=max_labels_dataset_shape,\n",
    "    compression=None)\n",
    "\n",
    "h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dataset to h5 for further using.\n",
    "def extracting_features(dataset_origin, index, batch_size):\n",
    "    cur = index * 32\n",
    "    batch_features = [dataset_origin.__getitem__(idx) for idx in range(cur,cur + batch_size)] \n",
    "    train_data,train_label = zip(*batch_features)\n",
    "    train_data,train_label = np.array(list(train_data)),np.array(list(train_label))\n",
    "    train_data = np.transpose(train_data, axes=(0, 2, 3, 4, 1))\n",
    "    return train_data,train_label\n",
    "\n",
    "def get_label_index(vocab_by_index, index_by_vocab, label):\n",
    "    label_index = index_by_vocab.get(label, -1)\n",
    "    if label_index > -1:\n",
    "        return label_index\n",
    "    else:\n",
    "        label_index = len(vocab_by_index)\n",
    "        index_by_vocab[label] = label_index\n",
    "        vocab_by_index.append(label)\n",
    "        return label_index\n",
    "    \n",
    "    \n",
    "def save_to_h5(h5_file,features,labels,index,batch_size):\n",
    "    assert len(features)==batch_size,\"Length of features and batch_size is not equal, can't perform saving operation\"\n",
    "    assert len(labels)==batch_size, \"Length of labels and batch_size is not equal, can't perform saving operation\"\n",
    "    features_set = h5_file['features']\n",
    "    labels_set = h5_file['labels']\n",
    "    cur = index*batch_size\n",
    "    new_features_shape=(\n",
    "        cur+batch_size,\n",
    "        basic_features_dataset_shape[1],\n",
    "        basic_features_dataset_shape[2],\n",
    "        basic_features_dataset_shape[3],\n",
    "        basic_features_dataset_shape[4],\n",
    "    )\n",
    "    new_labels_shape=(\n",
    "        cur+batch_size,\n",
    "        basic_labels_dataset_shape[1]\n",
    "    )\n",
    "    features_set.resize(new_features_shape)\n",
    "    labels_set.resize(new_labels_shape)\n",
    "    \n",
    "    features_set[cur:cur+batch_size] = features\n",
    "    labels_set[cur:cur+batch_size] = labels\n",
    "    \n",
    "def combine_process(dataset,h5_file,index,batch_size,vocab_by_index, index_by_vocab):\n",
    "    features,labels = extracting_features(dataset,index,batch_size)\n",
    "    labels = [get_label_index(vocab_by_index, index_by_vocab, label) for label in labels]\n",
    "    labels = to_categorical(labels,num_classes=num_classes)\n",
    "    save_to_h5(h5_file,features,labels,index,batch_size)\n",
    "    \n",
    "    \n",
    "def processing_dataset(dataset,h5_file,length,batch_size=32):\n",
    "    # Distinct Label Set\n",
    "    vocab_by_index = []\n",
    "    index_by_vocab = {}\n",
    "    num_of_batches = length // batch_size\n",
    "    print('Num of Batches: %d'%num_of_batches)\n",
    "    fit_batch = False\n",
    "    if num_of_batches*32 == length:\n",
    "        fit_batch = True\n",
    "    for index in range(num_of_batches):\n",
    "        print(\"##############################\")\n",
    "        print(\"############ %d/%d #############\"%(index,num_of_batches))\n",
    "        combine_process(dataset,h5_file,index,batch_size,vocab_by_index, index_by_vocab)\n",
    "        print(\"##############################\")\n",
    "#     if not fit_batch:\n",
    "#         combine_process(dataset,h5_file,num_of_batches,batch_size,vocab_by_index, index_by_vocab)\n",
    "    return (vocab_by_index,index_by_vocab)\n",
    "    \n",
    "    # Batch Reading and Saving to H5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CASES\n",
    "# processing_dataset(dataset,length_of_dataset,h5_file)\n",
    "# extracting_features(dataset, 100, 32)\n",
    "# vocab_by_index, index_by_vocab = ['01001','01234'], {'01001':0,'01234':1}\n",
    "# print(get_label_index(vocab_by_index, index_by_vocab, '0000'))\n",
    "# print(vocab_by_index, index_by_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do processing to h5 file\n",
    "try:\n",
    "    h5_file = h5.File(final_file, 'a')\n",
    "    vocab=processing_dataset(dataset_origin,h5_file,length_of_dataset,512)\n",
    "finally:\n",
    "    if h5_file:\n",
    "        h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    h5_file = h5.File(final_file, 'r')\n",
    "    print(h5_file['features'])\n",
    "    print(h5_file['labels'][117247])\n",
    "finally:\n",
    "    if h5_file:\n",
    "        h5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Model API Related\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam,SGD\n",
    "\n",
    "# Layers\n",
    "from keras.layers import Input,Conv3D,PReLU,MaxPool3D,Flatten,Dense,Activation\n",
    "\n",
    "# vis tool\n",
    "from keras.utils.vis_utils import model_to_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "def model(input_shape, num_class):\n",
    "    inputs=Input(shape=input_shape,name=\"input-layer\")\n",
    "    \n",
    "    # Conv 1\n",
    "    X = Conv3D(filters=16, kernel_size=(3, 1, 5), strides=(1, 1, 1), name=\"conv1-1\")(inputs)\n",
    "    X = PReLU(name=\"activation1-1\")(X)\n",
    "    X = Conv3D(filters=16, kernel_size=(3, 9, 1),strides=(1, 2, 1),name=\"conv1-2\")(X)\n",
    "    X = PReLU(name=\"activation1-2\")(X)\n",
    "    X = MaxPool3D(pool_size=(1, 1, 2), strides=(1, 1, 2), padding=\"valid\", name=\"pool-1\")(X)\n",
    "    \n",
    "    # Conv 2\n",
    "    X = Conv3D(filters=16, kernel_size=(3, 1, 4), strides=(1, 1, 1), name=\"conv2-1\")(X)\n",
    "    X = PReLU(name=\"activation2-1\")(X)\n",
    "    X = Conv3D(filters=16, kernel_size=(3, 8, 1),strides=(1, 2, 1),name=\"conv2-2\")(X)\n",
    "    X = PReLU(name=\"activation2-2\")(X)\n",
    "    X = MaxPool3D(pool_size=(1, 1, 2), strides=(1, 1, 2), padding=\"valid\", name=\"pool-2\")(X)\n",
    "    \n",
    "    # Conv 3\n",
    "    X = Conv3D(filters=16, kernel_size=(3, 1, 3), strides=(1, 1, 1), name=\"conv3-1\")(X)\n",
    "    X = PReLU(name=\"activation3-1\")(X)\n",
    "    X = Conv3D(filters=16, kernel_size=(3, 7, 1),strides=(1, 1, 1),name=\"conv3-2\")(X)\n",
    "    X = PReLU(name=\"activation3-2\")(X)\n",
    "    \n",
    "    # Conv 4\n",
    "    X = Conv3D(filters=16, kernel_size=(3, 1, 3), strides=(1, 1, 1), name=\"conv4-1\")(X)\n",
    "    X = PReLU(name=\"activation4-1\")(X)\n",
    "    X = Conv3D(filters=16, kernel_size=(3, 7, 1),strides=(1, 1, 1),name=\"conv4-2\")(X)\n",
    "    X = PReLU(name=\"activation4-2\")(X)\n",
    "    \n",
    "    # Flaten\n",
    "    X = Flatten()(X)\n",
    "\n",
    "    # FC\n",
    "    X = Dense(units=128,name=\"fc\",activation='relu')(X)\n",
    "#     X = PReLU(name=\"fc-ac\")(X)\n",
    "    feature_model = Model(inputs=inputs,outputs=X)\n",
    "    # Final Activation\n",
    "    X = Dense(units=num_class,activation='softmax',name=\"ac_softmax\")(X)\n",
    "    model = Model(inputs=inputs,outputs=X)\n",
    "    \n",
    "    return model,feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,feature_model = model((20,80,40,1),num_classes)\n",
    "opt = Adam(beta_1=0.9, beta_2=0.999, decay=1e-6, lr=0.01)\n",
    "# opt = SGD(lr=0.01)\n",
    "# opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 20, 80, 40, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv1-1 (Conv3D)             (None, 18, 80, 36, 16)    256       \n",
      "_________________________________________________________________\n",
      "activation1-1 (PReLU)        (None, 18, 80, 36, 16)    829440    \n",
      "_________________________________________________________________\n",
      "conv1-2 (Conv3D)             (None, 16, 36, 36, 16)    6928      \n",
      "_________________________________________________________________\n",
      "activation1-2 (PReLU)        (None, 16, 36, 36, 16)    331776    \n",
      "_________________________________________________________________\n",
      "pool-1 (MaxPooling3D)        (None, 16, 36, 18, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv2-1 (Conv3D)             (None, 14, 36, 15, 16)    3088      \n",
      "_________________________________________________________________\n",
      "activation2-1 (PReLU)        (None, 14, 36, 15, 16)    120960    \n",
      "_________________________________________________________________\n",
      "conv2-2 (Conv3D)             (None, 12, 15, 15, 16)    6160      \n",
      "_________________________________________________________________\n",
      "activation2-2 (PReLU)        (None, 12, 15, 15, 16)    43200     \n",
      "_________________________________________________________________\n",
      "pool-2 (MaxPooling3D)        (None, 12, 15, 7, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3-1 (Conv3D)             (None, 10, 15, 5, 16)     2320      \n",
      "_________________________________________________________________\n",
      "activation3-1 (PReLU)        (None, 10, 15, 5, 16)     12000     \n",
      "_________________________________________________________________\n",
      "conv3-2 (Conv3D)             (None, 8, 9, 5, 16)       5392      \n",
      "_________________________________________________________________\n",
      "activation3-2 (PReLU)        (None, 8, 9, 5, 16)       5760      \n",
      "_________________________________________________________________\n",
      "conv4-1 (Conv3D)             (None, 6, 9, 3, 16)       2320      \n",
      "_________________________________________________________________\n",
      "activation4-1 (PReLU)        (None, 6, 9, 3, 16)       2592      \n",
      "_________________________________________________________________\n",
      "conv4-2 (Conv3D)             (None, 4, 3, 3, 16)       5392      \n",
      "_________________________________________________________________\n",
      "activation4-2 (PReLU)        (None, 4, 3, 3, 16)       576       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "ac_softmax (Dense)           (None, 695)               89655     \n",
      "=================================================================\n",
      "Total params: 1,541,671\n",
      "Trainable params: 1,541,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "model.summary()\n",
    "print(\"#### Feature Model Below ####\")\n",
    "feature_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 117248 samples, validate on 6144 samples\n",
      "Epoch 1/1\n",
      "117184/117248 [============================>.] - ETA: 2s - loss: 3.5116 - acc: 0.0647"
     ]
    }
   ],
   "source": [
    "test_final_name='dataset_test.h5'\n",
    "try:\n",
    "    h5_file = h5.File(final_file, 'r')\n",
    "    h5_file_test = h5.File(test_final_name, 'r')\n",
    "    train_data,train_label=h5_file['features'],h5_file['labels']\n",
    "    test_data,test_label=h5_file_test['features'],h5_file_test['labels']\n",
    "    history=model.fit(batch_size=64,\n",
    "              epochs=1,\n",
    "              shuffle=\"batch\",\n",
    "              x=train_data,\n",
    "              y=train_label,validation_data=(test_data,test_label))\n",
    "finally:\n",
    "    if h5_file:\n",
    "        h5_file.close()\n",
    "    if h5_file_test:\n",
    "        h5_file_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if h5_file_test:\n",
    "    h5_file_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/model-0911.h5\")\n",
    "# model.save_weights(\"model-weights-\" + time_str + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation on softmax\n",
    "# Test a split of dataset.\n",
    "test_final_name='dataset_test.h5'\n",
    "try:\n",
    "    h5_file_test = h5.File(test_final_name, 'r')\n",
    "    test_data,test_label=h5_file_test['features'][:],h5_file_test['labels'][:]\n",
    "    evaluation = model.evaluate(batch_size=64,x=test_data,y=test_label)\n",
    "    metrics_name = model.metrics_names\n",
    "    eval_results = list([*zip(metrics_name, evaluation)])\n",
    "    print(eval_results)\n",
    "finally:\n",
    "    if h5_file_test:\n",
    "        h5_file_test.close()\n",
    "    test_data,test_label=None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo, enrollment and similarity comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalution the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-py3",
   "language": "python",
   "name": "keras-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
